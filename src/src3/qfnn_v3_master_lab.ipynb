{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aea6957",
   "metadata": {},
   "source": [
    "# üåå QFNN v3 Master Lab\n",
    "---\n",
    "Full physics-based sequence model training + dynamic dreaming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b00ca3c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device Ready: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from axiomatic_qfnn_v3 import AxiomaticQuantumFieldV3\n",
    "from qfnn_v3_dynamic_generator_fixed import QFNNFieldExpanderV3\n",
    "from trainer_qfnn_v3 import train_qfnn_v3\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Device Ready:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf108d2",
   "metadata": {},
   "source": [
    "## üß† Load QFNN Model Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd7c6a3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "qfnn = AxiomaticQuantumFieldV3()\n",
    "expander = QFNNFieldExpanderV3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f8a8c",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29505bd8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 9749.04 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:03<00:00, 9956.70 examples/s] \n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 10291.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1:0] Loss: 0.9219 | Entropy: 1.5578\n",
      "[1:50] Loss: 0.9584 | Entropy: 1.5498\n",
      "[1:100] Loss: 1.0181 | Entropy: 1.5499\n",
      "[1:150] Loss: 1.0263 | Entropy: 1.5476\n",
      "[1:200] Loss: 1.0226 | Entropy: 1.5464\n",
      "[1:250] Loss: 1.0224 | Entropy: 1.5461\n",
      "[1:300] Loss: 1.0216 | Entropy: 1.5459\n",
      "[1:350] Loss: 1.0227 | Entropy: 1.5478\n",
      "[1:400] Loss: 1.0218 | Entropy: 1.5469\n",
      "[1:450] Loss: 1.0211 | Entropy: 1.5464\n",
      "[3:0] Loss: 0.8861 | Entropy: 4.7631\n",
      "[3:50] Loss: 1.0619 | Entropy: 4.7588\n",
      "[3:100] Loss: 1.1678 | Entropy: 4.7574\n",
      "[3:150] Loss: 1.2165 | Entropy: 4.7581\n",
      "[3:200] Loss: 1.2364 | Entropy: 4.7604\n",
      "[3:250] Loss: 1.2445 | Entropy: 4.7552\n",
      "[3:300] Loss: 0.9512 | Entropy: 4.7535\n",
      "[3:350] Loss: 0.8478 | Entropy: 4.7503\n",
      "[3:400] Loss: 0.9368 | Entropy: 4.7512\n",
      "[3:450] Loss: 0.9238 | Entropy: 4.7518\n",
      "[4:0] Loss: 0.8850 | Entropy: 4.4305\n",
      "[4:50] Loss: 1.0569 | Entropy: 4.4325\n",
      "[4:100] Loss: 1.1403 | Entropy: 4.4343\n",
      "[4:150] Loss: 1.1627 | Entropy: 4.4326\n",
      "[4:200] Loss: 1.1647 | Entropy: 4.4311\n",
      "[4:250] Loss: 1.1600 | Entropy: 4.4287\n",
      "[4:300] Loss: 0.9729 | Entropy: 4.4261\n",
      "[4:350] Loss: 0.9175 | Entropy: 4.4249\n",
      "[4:400] Loss: 0.8734 | Entropy: 4.4250\n",
      "[4:450] Loss: 0.8455 | Entropy: 4.4235\n",
      "[5:0] Loss: 0.8870 | Entropy: 4.5345\n",
      "[5:50] Loss: 1.0595 | Entropy: 4.5333\n",
      "[5:100] Loss: 1.1492 | Entropy: 4.5345\n",
      "[5:150] Loss: 1.1810 | Entropy: 4.5327\n",
      "[5:200] Loss: 1.1876 | Entropy: 4.5311\n",
      "[5:250] Loss: 1.1268 | Entropy: 4.5321\n",
      "[5:300] Loss: 0.9173 | Entropy: 4.5331\n",
      "[5:350] Loss: 0.9138 | Entropy: 4.5320\n",
      "[5:400] Loss: 0.8448 | Entropy: 4.5310\n",
      "[5:450] Loss: 0.8838 | Entropy: 4.5309\n",
      "üñºÔ∏è Training Progress saved to ./trained_model_v3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train (WikiText2 default)\n",
    "train_qfnn_v3(dataset_name=\"wiki2\", model_name=\"gpt2\", output_dir=\"./trained_model_v3\", epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678326e",
   "metadata": {},
   "source": [
    "## üåå Dynamic Dreaming: Inference Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3833b675",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå† Dream Expansion: ['truth', 'emerges', 'from', 'the', 'hidden', 'Œ∏=-2.35', 'Œ∏=-2.50', 'Œ∏=-2.30', 'Œ∏=-0.67', 'Œ∏=-3.09', 'Œ∏=1.06', 'Œ∏=0.62', 'Œ∏=-0.02', 'Œ∏=2.65', 'Œ∏=-1.51', 'Œ∏=1.36', 'Œ∏=0.92', 'Œ∏=1.61', 'Œ∏=2.75', 'Œ∏=-1.11', 'Œ∏=1.31', 'Œ∏=2.10', 'Œ∏=0.67', 'Œ∏=2.50', 'Œ∏=1.46', 'Œ∏=-1.41', 'Œ∏=0.67', 'Œ∏=0.82', 'Œ∏=2.80', 'Œ∏=-0.96', 'Œ∏=-2.00', 'Œ∏=2.75', 'Œ∏=-1.16', 'Œ∏=1.86', 'Œ∏=1.56']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Seed\n",
    "prompt = [\"truth\", \"emerges\", \"from\", \"the\", \"hidden\"]\n",
    "\n",
    "# Expand dream\n",
    "dream_sequence = expander.generate_sequence(prompt, num_new_tokens=30)\n",
    "\n",
    "print(\"üå† Dream Expansion:\", dream_sequence)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf53660",
   "metadata": {},
   "source": [
    "## üìà Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf294e37",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics not found: [Errno 2] No such file or directory: './trained_model_v3/training_progress.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load training history if available\n",
    "try:\n",
    "    history = pd.read_csv(\"./trained_model_v3/training_progress.csv\")\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(history[\"loss\"], label=\"Loss\")\n",
    "    plt.plot(history[\"entropy\"], label=\"Entropy\")\n",
    "    plt.title(\"Training Loss & Entropy Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Training metrics not found:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd0c6a",
   "metadata": {},
   "source": [
    "## üî¨ (Optional) HyperTune Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06563a0d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# You can later load and run hyper-tune lab to optimize Œ≤, step_size, hebb_lr etc.\n",
    "# (Refer to HyperTune Lab separately for structured sweeps.)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
