# quantum_field_engine_monolithic.py
import torch
import math
from nltk.tokenize import word_tokenize
from text_decoder import field_to_sentence, decode_from_field
import torch.nn.functional as F
import nltk
nltk.download("punkt")
nltk.download('averaged_perceptron_tagger_eng')
nltk.download("averaged_perceptron_tagger")
nltk.download("wordnet")

# --- CONFIG ---
Î¦ = (1 + math.sqrt(5)) / 2
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CONFIG = {
    "Î·_0": 0.002,
    "Î»": 0.75,           # phase loss to learning rate sensitivity
    "Î¼": 0.2,            # memory modulation coefficient
    "hebb_lr": 0.05,
    "hebb_decay": 0.99,
    "entropy_weight": 0.01,
    "pareto_k": 0.25,
    "h_base": 0.01,
    "steps": 500,
    "epsilon": 1e-8,
}

# --- TOKENIZER ---
def tokenize(text):
    return word_tokenize(text.lower())

# --- INIT FIELD ---
def init_field(tokens):
    N = len(tokens)
    i = torch.arange(N, dtype=torch.float32, device=DEVICE)
    Î¸ = (i * Î¦ * 2 * torch.pi) % (2 * torch.pi)
    r = 0.3 + 0.7 * torch.linspace(0, 1, N, device=DEVICE)
    Ïˆ = torch.randn(N, 2, device=DEVICE)
    Ïˆ = torch.view_as_complex(Ïˆ)
    W = torch.zeros((N, N), dtype=torch.cfloat, device=DEVICE)
    return Î¸, r, Ïˆ, W

# --- LOSS + ENTROPY ---
def compute_loss(Ïˆ, Î¸_pred, Î¸_target):
    phase_loss = torch.mean((Î¸_pred - Î¸_target) ** 2)
    mag_sq = torch.abs(Ïˆ) ** 2
    entropy = -torch.sum(mag_sq * torch.log(mag_sq + CONFIG["epsilon"]))
    return phase_loss, entropy

# --- PARETO MASK ---
def pareto_mask(metric, k):
    threshold = torch.quantile(metric, 1 - k)
    return metric >= threshold

# --- HEBBIAN UPDATE ---
def update_hebbian(W, Ïˆ, Î·, decay):
    Î”W = Î· * torch.outer(Ïˆ.conj(), Ïˆ)
    return decay * W + Î”W

# --- SCHRÃ–DINGER PROPAGATION ---

def propagate(Ïˆ, V, h):
    # First Euler step
    k1 = -Ïˆ + V * Ïˆ
    Ïˆ_mid = Ïˆ + h * k1
    # Normalize midpoint
    Ïˆ_mid = F.normalize(Ïˆ_mid, p=2, dim=0)  # unit-norm across field
    # Recalculate derivative at midpoint
    k2 = -Ïˆ_mid + V * Ïˆ_mid
    # Final Heun step
    Ïˆ_next = Ïˆ + 0.5 * h * (k1 + k2)
    # Final normalization (unit-energy)
    Ïˆ_next = F.normalize(Ïˆ_next, p=2, dim=0)

    return Ïˆ_next

    
# --- Shock with cohernece approaching .99---

def apply_inverse_beta_shock(Ïˆ, step, T, beta_min=0.1, beta_max=20.0, Î±=2.0, Î´=0.1, Î¦=(1 + math.sqrt(5)) / 2):
    Î² = beta_min + (beta_max - beta_min) * (step / T)**Î± * (1 + Î´ * math.sin(2 * math.pi * Î¦ * step / T))
    noise = (1.0 / Î²) * torch.randn_like(Ïˆ)
    return Ïˆ + noise

# --- COUPLED SYSTEM EVOLUTION ---
def evolve(text):
    tokens = tokenize(text)
    N = len(tokens)
    Î¸, r, Ïˆ, W = init_field(tokens)
    F_stack = torch.zeros_like(Ïˆ)
    Î¸_target = torch.roll(Î¸, shifts=-1)

    for step in range(CONFIG["steps"]):
        Î¸_pred = torch.angle(Ïˆ)
        Î¸_error = torch.abs(Î¸_pred - Î¸_target)

        phase_loss = torch.mean(Î¸_error ** 2)
        mag_sq = torch.abs(Ïˆ) ** 2
        entropy = -torch.sum(mag_sq * torch.log(mag_sq + CONFIG["epsilon"]))
        loss_total = phase_loss + CONFIG["entropy_weight"] * entropy

        # Vectorized Pareto top-k mask
        k = int(CONFIG["pareto_k"] * N)
        topk = torch.topk(Î¸_error, k=k)
        mask = torch.zeros(N, dtype=torch.bool, device=DEVICE)
        mask[topk.indices] = True

        Ïˆ_selected = Ïˆ.clone()
        Ïˆ_selected[~mask] = Ïˆ.detach()[~mask]

        # Vectorized Î· update
        memory_pressure = torch.sum(torch.abs(W), dim=1)  # (N,)
        Î· = CONFIG["Î·_0"] + CONFIG["Î»"] * phase_loss + CONFIG["Î¼"] * memory_pressure  # (N,)
        Î· = Î·.unsqueeze(1)  # shape: (N,1)

        # Compute potential + memory field stack
        V = (W @ Ïˆ).real + CONFIG.get("stack_decay", 0.95) * F_stack.real
        Ïˆ = propagate(Ïˆ_selected, V, CONFIG["h_base"] * Î·.squeeze())  # Î· is per-token

        # Hebbian update (in-place)
        Î”W = CONFIG["hebb_lr"] * torch.outer(Ïˆ.conj(), Ïˆ)
        W.mul_(CONFIG["hebb_decay"]).add_(Î”W)

        # Update memory trace
        F_stack = CONFIG.get("stack_decay", 0.95) * F_stack + Ïˆ

        # Logging
        if step % 50 == 0 or step == CONFIG["steps"] - 1:
            coherence = torch.abs(torch.mean(torch.exp(1j * Î¸_pred)))
            if coherence > 0.99:
                Ïˆ = apply_inverse_beta_shock(Ïˆ, step, CONFIG["steps"])
                print(f"âš¡ Entropy shock injected at step {step} | Î² adjusted for phase exploration.")
            print(f"[{step}] Loss: {loss_total:.4f} | Phase: {phase_loss:.4f} | Entropy: {entropy:.4f} | Coherence: {coherence:.4f}")
            print(f"    Î· mean: {Î·.mean().item():.4f} | F_stack norm: {F_stack.norm().item():.4f}")


    return Ïˆ, W, F_stack, Î¸_target

def compose_fields(fields, method='average'):
    assert len(fields) > 1, "Need at least two fields to compose."

    shapes = [f["Ïˆ"].shape for f in fields]
    if not all(s == shapes[0] for s in shapes):
        raise ValueError("All fields must have the same Ïˆ dimensionality.")

    Ïˆ_stack = torch.stack([f["Ïˆ"].to(DEVICE) for f in fields])
    W_stack = torch.stack([f["W"].to(DEVICE) for f in fields])
    F_stack = torch.stack([f["F_stack"].to(DEVICE) for f in fields])

    if method == 'average':
        Ïˆ_fused = Ïˆ_stack.mean(dim=0)
        W_fused = W_stack.mean(dim=0)
        F_fused = F_stack.mean(dim=0)
    elif method == 'sum':
        Ïˆ_fused = Ïˆ_stack.sum(dim=0)
        W_fused = W_stack.sum(dim=0)
        F_fused = F_stack.sum(dim=0)
    elif method == 'max':
        Ïˆ_fused = Ïˆ_stack.max(dim=0).values
        W_fused = W_stack.max(dim=0).values
        F_fused = F_stack.max(dim=0).values
    else:
        raise ValueError("Unknown composition method.")

    # Decode the fused field
    Î¸_target = torch.roll(torch.angle(Ïˆ_fused), shifts=-1)
    decoded_tokens = decode_from_field(torch.angle(Ïˆ_fused), Î¸_target)
    decoded_sentence = field_to_sentence(decoded_tokens)

    return {
        "Ïˆ": Ïˆ_fused,
        "W": W_fused,
        "F_stack": F_fused,
        "decoded": decoded_tokens,
        "sentence": decoded_sentence
    }

def run_qfnn_on_corpus(text, window_size=32, stride=16):
    from nltk.tokenize import sent_tokenize

    checkpoints = []
    sentences = sent_tokenize(text)
    current = ""
    
    for sentence in sentences:
        if len(current.split()) < window_size:
            current += " " + sentence
        else:
            evolve_from_text(current.strip())
            checkpoint = torch.load("semantic_field_checkpoint.pt")
            checkpoints.append(checkpoint)
            current = sentence
    
    if current.strip():
        evolve_from_text(current.strip())
        checkpoint = torch.load("semantic_field_checkpoint.pt")
        checkpoints.append(checkpoint)
    
    return checkpoints

def save_checkpoint(path, Ïˆ, W, F_stack, meta=None):
    torch.save({
        "Ïˆ": Ïˆ.detach().cpu(),
        "W": W.detach().cpu(),
        "F_stack": F_stack.detach().cpu(),
        "meta": meta or {}
    }, path)
    print(f"ðŸ’¾ Field memory saved: {path}")

def load_checkpoint(path):
    checkpoint = torch.load(path, map_location=DEVICE)
    return checkpoint["Ïˆ"].to(DEVICE), checkpoint["W"].to(DEVICE), checkpoint["F_stack"].to(DEVICE), checkpoint.get("meta", {})


import matplotlib.pyplot as plt

def visualize_field(Ïˆ, title="Quantum Field Ïˆ"):
    amp = torch.abs(Ïˆ).cpu().numpy()
    phase = torch.angle(Ïˆ).cpu().numpy()
    fig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True)
    ax[0].plot(amp)
    ax[0].set_title("Amplitude")
    ax[1].plot(phase)
    ax[1].set_title("Phase")
    fig.suptitle(title)
    plt.tight_layout()
    plt.show()


def decode_stream_from_Ïˆ(Ïˆ, Î¸_target):
    Î¸_fused = torch.angle(fused["Ïˆ"])
    Î¸_target_fused = torch.roll(Î¸_fused, shifts=-1)
    decoded_tokens = decode_from_field(Î¸_fused, Î¸_target_fused)
    sentence = field_to_sentence(decoded_tokens)
    print("ðŸ§  Fused Field Decoded:", sentence)

    return sentence

def train_from_txt_file(path):
    with open(path, "r", encoding="utf-8") as f:
        text = f.read()
    return run_qfnn_on_corpus(text)

def evolve_from_text(text, save_path="semantic_field_checkpoint.pt"):
    Ïˆ, W, F_stack, _ = evolve(text)
    save_checkpoint(save_path, Ïˆ, W, F_stack)

# EXAMPLE: Compose 2 different field runs
evolve_from_text("a field of intelligence evolves")
cp1 = torch.load("semantic_field_checkpoint.pt")

evolve_from_text("memory and resonance organize coherence")
cp2 = torch.load("semantic_field_checkpoint.pt")

fused = compose_fields([cp1, cp2])
# Recalculate Î¸_target for the fused field
Î¸_target_fused = torch.roll(torch.angle(fused["Ïˆ"]), shifts=-1)

# Decode the sentence
decoded_tokens = decode_from_field(torch.angle(fused["Ïˆ"]), Î¸_target_fused)
sentence = field_to_sentence(decoded_tokens)

print("ðŸ§  Fused Field Decoded:", sentence)


# EXAMPLE: Run across a large text
long_text = open("out\input.txt").read()
checkpoints = run_qfnn_on_corpus(long_text)


# --- RUN ---
if __name__ == "__main__":
    text = "consciousness emerges from coherent interference patterns in the semantic quantum field"
    Ïˆ, W, F_stack, Î¸_target = evolve(text)
    # Save field memory
    
    save_checkpoint("coherent_memory.pt", Ïˆ, W, F_stack)


    Ïˆ, W, F_stack, meta = load_checkpoint("coherent_memory.pt")

    #Load & Decode It
    Î¸_target = torch.roll(torch.angle(Ïˆ), shifts=-1)
    decode_stream_from_Ïˆ(Ïˆ, Î¸_target)
    visualize_field(Ïˆ, title="Ïˆ from 'coherent memory' concept")

    checkpoints = train_from_txt_file("out\input.txt")
    visualize_field(checkpoints[0]["Ïˆ"], title="Field from Corpus Line 1")
